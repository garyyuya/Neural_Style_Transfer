{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# download coco dataset to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd  /content/drive/'My Drive'/CV_project/train\n",
        "\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "\n",
        "!unzip train2014.zip"
      ],
      "metadata": {
        "id": "ZF8nEoizxwIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change directory path\n",
        "%cd  /content/drive/'My Drive'/CV_project"
      ],
      "metadata": {
        "id": "8iciAZOyx0-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network:"
      ],
      "metadata": {
        "id": "nizS7uYf0W6J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GaOmrpDz7dG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules.instancenorm import InstanceNorm2d\n",
        "\n",
        "class TransformerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerNet, self).__init__()\n",
        "\n",
        "        self.ConvBlock = nn.Sequential(\n",
        "            ConvLayer(3, 32, 9, 1),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(32, 64, 3, 2),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(64, 128, 3, 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.ResidualBlock = nn.Sequential(\n",
        "            ResidualLayer(128, 3),\n",
        "            ResidualLayer(128, 3),\n",
        "            ResidualLayer(128, 3),\n",
        "            ResidualLayer(128, 3),\n",
        "            ResidualLayer(128, 3)\n",
        "        )\n",
        "\n",
        "        self.UpsampleBlock = nn.Sequential(\n",
        "            UpsampleConvLayer(128, 64, 3, 1, 2),\n",
        "            nn.InstanceNorm2d(64, affine=True),\n",
        "            UpsampleConvLayer(64, 32, 3, 1, 2),\n",
        "            nn.InstanceNorm2d(32, affine=True),\n",
        "            ConvLayer(32, 3, 9, 1, norm='None')\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ConvBlock(x)\n",
        "        x = self.ResidualBlock(x)\n",
        "        x = self.UpsampleBlock(x)\n",
        "        return x\n",
        "\n",
        "class TransformerResNextNetwork(nn.Module):\n",
        "    '''\n",
        "    Feedforward Transformation Network - ResNeXt\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(TransformerResNextNetwork, self).__init__()\n",
        "        self.ConvBlock = nn.Sequential(\n",
        "            ConvLayer(3, 32, 9, 1),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(32, 64, 3, 2),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(64, 128, 3, 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.ResidualBlock = nn.Sequential(\n",
        "            ResNextLayer(128, [64, 64, 128], kernel_size=3),\n",
        "            ResNextLayer(128, [64, 64, 128], kernel_size=3),\n",
        "            ResNextLayer(128, [64, 64, 128], kernel_size=3),\n",
        "            ResNextLayer(128, [64, 64, 128], kernel_size=3),\n",
        "            ResNextLayer(128, [64, 64, 128], kernel_size=3)\n",
        "        )\n",
        "        self.DeconvBlock = nn.Sequential(\n",
        "            DeconvLayer(128, 64, 3, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            DeconvLayer(64, 32, 3, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(32, 3, 9, 1, norm='None')\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ConvBlock(x)\n",
        "        x = self.ResidualBlock(x)\n",
        "        out = self.DeconvBlock(x)\n",
        "        return out\n",
        "\n",
        "class TransformerNetworkDenseNet(nn.Module):\n",
        "    '''\n",
        "    Feedforward Transformer Network using DenseNet Block instead of Residual Block\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(TransformerNetworkDenseNet, self).__init__()\n",
        "        self.ConvBlock = nn.Sequential(\n",
        "            ConvLayerNB(3, 32, 9, 1),\n",
        "            nn.ReLU(),\n",
        "            ConvLayerNB(32, 64, 3, 2),\n",
        "            nn.ReLU(),\n",
        "            ConvLayerNB(64, 128, 3, 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.DenseBlock = nn.Sequential(\n",
        "            NormReluConv(128, 64, 1, 1),\n",
        "            DenseLayerBottleNeck(64, 16),\n",
        "            DenseLayerBottleNeck(80, 16),\n",
        "            DenseLayerBottleNeck(96, 16),\n",
        "            DenseLayerBottleNeck(112, 16)\n",
        "        )\n",
        "        self.DeconvBlock = nn.Sequential(\n",
        "            DeconvLayer(128, 64, 3, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            DeconvLayer(64, 32, 3, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            ConvLayer(32, 3, 9, 1, norm='None')\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ConvBlock(x)\n",
        "        x = self.DenseBlock(x)\n",
        "        out = self.DeconvBlock(x)\n",
        "        return out\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, norm='instance'):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        # padding layer\n",
        "        padding_size = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(padding_size)\n",
        "\n",
        "        # convolution layer\n",
        "        self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "        # normalization layer\n",
        "        self.norm_type = norm\n",
        "        if (norm == 'instance'):\n",
        "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
        "        elif (norm == 'batch'):\n",
        "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reflection_pad(x)\n",
        "        x = self.conv_layer(x)\n",
        "        if (self.norm_type == 'None'):\n",
        "            out = x\n",
        "        else:\n",
        "            out = self.norm_layer(x)\n",
        "        return out\n",
        "\n",
        "class ConvLayerNB(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, norm='instance'):\n",
        "        super(ConvLayerNB, self).__init__()\n",
        "        # Padding Layers\n",
        "        padding_size = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(padding_size)\n",
        "\n",
        "        # Convolution Layer\n",
        "        self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, bias=False)\n",
        "\n",
        "        # Normalization Layers\n",
        "        self.norm_type = norm\n",
        "        if (norm == 'instance'):\n",
        "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
        "        elif (norm == 'batch'):\n",
        "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reflection_pad(x)\n",
        "        x = self.conv_layer(x)\n",
        "        if (self.norm_type == 'None'):\n",
        "            out = x\n",
        "        else:\n",
        "            out = self.norm_layer(x)\n",
        "        return out\n",
        "\n",
        "class NormLReluConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(NormLReluConv, self).__init__()\n",
        "\n",
        "        # Normalization Layers\n",
        "        self.norm_layer = nn.InstanceNorm2d(in_channels, affine=True)\n",
        "\n",
        "        # ReLU Layer\n",
        "        self.relu_layer = nn.ReLU()\n",
        "\n",
        "        # Padding Layers\n",
        "        padding_size = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(padding_size)\n",
        "\n",
        "        # Convolution Layer\n",
        "        self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm_layer(x)\n",
        "        x = self.relu_layer(x)\n",
        "        x = self.reflection_pad(x)\n",
        "        x = self.conv_layer(x)\n",
        "        return x\n",
        "\n",
        "class NormReluConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, norm='instance'):\n",
        "        super(NormReluConv, self).__init__()\n",
        "\n",
        "        # Normalization Layers\n",
        "        if (norm == 'instance'):\n",
        "            self.norm_layer = nn.InstanceNorm2d(in_channels, affine=True)\n",
        "        elif (norm == 'batch'):\n",
        "            self.norm_layer = nn.BatchNorm2d(in_channels, affine=True)\n",
        "\n",
        "        # ReLU Layer\n",
        "        self.relu_layer = nn.ReLU()\n",
        "\n",
        "        # Padding Layers\n",
        "        padding_size = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(padding_size)\n",
        "\n",
        "        # Convolution Layer\n",
        "        self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm_layer(x)\n",
        "        x = self.relu_layer(x)\n",
        "        x = self.reflection_pad(x)\n",
        "        x = self.conv_layer(x)\n",
        "        return x\n",
        "\n",
        "class ResidualLayer(nn.Module):\n",
        "    '''\n",
        "    Reference: Deep Residual Learning for Image Recognition\n",
        "    https://arxiv.org/abs/1512.03385\n",
        "    '''\n",
        "    def __init__(self, channels=128, kernel_size=3):\n",
        "        super(ResidualLayer, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x                        # preserve residual\n",
        "        out = self.relu(self.conv1(x))      # 1st conv layer and activation\n",
        "        out = self.conv2(out)               # 2nd conv layer\n",
        "        out = out + residual                # add in residual\n",
        "        return out\n",
        "\n",
        "class ResNextLayer(nn.Module):\n",
        "    '''\n",
        "    Aggregated Residual Transformations for Deep Neural Networks\n",
        "        Equal to better performance with 10x less parameters\n",
        "    https://arxiv.org/abs/1611.05431\n",
        "    '''\n",
        "    def __init__(self, in_ch=128, channels=[64, 64, 128], kernel_size=3):\n",
        "        super(ResNextLayer, self).__init__()\n",
        "        ch1, ch2, ch3 = channels\n",
        "        self.conv1 = ConvLayer(in_ch, ch1, kernel_size=1, stride=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = ConvLayer(ch1, ch2, kernel_size=kernel_size, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.conv3 = ConvLayer(ch2, ch3, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu1(self.conv1(x))\n",
        "        out = self.relu2(self.conv2(out))\n",
        "        out = self.conv3(out)\n",
        "        out = out + identity\n",
        "        return out\n",
        "\n",
        "class UpsampleConvLayer(nn.Module):\n",
        "    '''\n",
        "    Upsamples the input and then does a convolution. This method gives better results\n",
        "    compared to ConvTranspose2d.\n",
        "    Reference: http://distill.pub/2016/deconv-checkerboard/\n",
        "    '''\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(UpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        padding_size = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(padding_size)\n",
        "        self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.upsample:\n",
        "            x_in = nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
        "        out = self.reflection_pad(x_in)\n",
        "        out = self.conv_layer(out)\n",
        "        return out\n",
        "\n",
        "class DeconvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding, norm='instance'):\n",
        "        super(DeconvLayer, self).__init__()\n",
        "\n",
        "        # Transposed Convolution\n",
        "        padding_size = kernel_size // 2\n",
        "        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding_size, output_padding)\n",
        "\n",
        "        # Normalization Layers\n",
        "        self.norm_type = norm\n",
        "        if (norm == 'instance'):\n",
        "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
        "        elif (norm == 'batch'):\n",
        "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_transpose(x)\n",
        "        if (self.norm_type == 'None'):\n",
        "            out = x\n",
        "        else:\n",
        "            out = self.norm_layer(x)\n",
        "        return out\n",
        "\n",
        "class DenseLayerBottleNeck(nn.Module):\n",
        "    '''\n",
        "    NORM - RELU - CONV1 -> NORM - RELU - CONV3\n",
        "    out_channels = Growth Rate\n",
        "    '''\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DenseLayerBottleNeck, self).__init__()\n",
        "\n",
        "        self.conv1 = NormLReluConv(in_channels, 4*out_channels, 1, 1)\n",
        "        self.conv3 = NormLReluConv(4*out_channels, out_channels, 3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv3(self.conv1(x))\n",
        "        out = torch.cat((x, out), 1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG:"
      ],
      "metadata": {
        "id": "YvsNLH0c0TCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        pretrained_features = models.vgg16(pretrained=True).features\n",
        "        self.layer1 = nn.Sequential()\n",
        "        self.layer2 = nn.Sequential()\n",
        "        self.layer3 = nn.Sequential()\n",
        "        self.layer4 = nn.Sequential()\n",
        "        for i in range(4):\n",
        "            self.layer1.add_module(str(i), pretrained_features[i])\n",
        "        for i in range(4, 9):\n",
        "            self.layer2.add_module(str(i), pretrained_features[i])\n",
        "        for i in range(9, 16):\n",
        "            self.layer3.add_module(str(i), pretrained_features[i])\n",
        "        for i in range(16, 23):\n",
        "            self.layer4.add_module(str(i), pretrained_features[i])\n",
        "\n",
        "        # Disable gradient history\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        x = self.layer1(X)\n",
        "        x_relu1_2 = x\n",
        "        x = self.layer2(x)\n",
        "        x_relu2_2 = x\n",
        "        x = self.layer3(x)\n",
        "        x_relu3_3 = x\n",
        "        x = self.layer4(x)\n",
        "        x_relu4_3 = x\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3'])\n",
        "        out = vgg_outputs(x_relu1_2, x_relu2_2, x_relu3_3, x_relu4_3)\n",
        "        return out"
      ],
      "metadata": {
        "id": "uVKSAngu0IE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Util:"
      ],
      "metadata": {
        "id": "C-QGbZOq0Pad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def load_image(filename):\n",
        "    img = Image.open(filename).convert('RGB')\n",
        "    return img\n",
        "\n",
        "def show_image(img):\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.imshow(img)\n",
        "\n",
        "def save_image(filename, data):\n",
        "    img = data.clone().clamp(0, 255).numpy()\n",
        "    img = img.transpose(1, 2, 0).astype(\"uint8\")\n",
        "    img = Image.fromarray(img)\n",
        "    img.save(filename)\n",
        "\n",
        "def gram_matrix(tensor):\n",
        "    b, ch, h, w = tensor.shape\n",
        "    x = tensor.view(b, ch, h * w)\n",
        "    x_t = x.transpose(1,2)\n",
        "    return torch.bmm(x, x_t) / (ch * h * w)\n",
        "\n",
        "def normalize_batch(batch):\n",
        "    mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
        "    std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
        "    batch = batch.div_(255.0)\n",
        "    return (batch - mean) / std"
      ],
      "metadata": {
        "id": "x6aQaH0T0MJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Style and Main:\n",
        "\n",
        "Style Image:\n",
        "python neural_style/neural_style.py eval --content-image </path/to/content/image> --model </path/to/saved/model> --output-image </path/to/output/image> --cuda 0\n",
        "\n",
        "Train Model:\n",
        "python neural_style/neural_style.py train --dataset </path/to/train-dataset> --style-image </path/to/style/image> --save-model-dir </path/to/save-model/folder> --epochs 2 --cuda 1\n",
        "\n"
      ],
      "metadata": {
        "id": "WaM7B5wS0gpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# paths to style image, content image, and directory to save model to\n",
        "style_image_path = './style_image/mosaic.jpg'\n",
        "content_image_path = './content_image/amber.jpg'\n",
        "model_path = './models/model_mosaic.pth'\n",
        "train_image_path = './coco_dataset'\n",
        "\n",
        "# path to save output image to\n",
        "output_image_path = './output/mosaic_amber.jpg'"
      ],
      "metadata": {
        "id": "zYjl24KsRUwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set number of epochs\n",
        "num_epochs = 2"
      ],
      "metadata": {
        "id": "2piUaC7ZR8AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set batch size\n",
        "batch_size = 4"
      ],
      "metadata": {
        "id": "WaXNd1HwTYEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set learning rate\n",
        "learning_rate = 1e-3"
      ],
      "metadata": {
        "id": "1AkjGSlyUISq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set log interval: number of images after which the training loss is logged\n",
        "log_interval = 500"
      ],
      "metadata": {
        "id": "0NS_soGdVH8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set checkpoint interval: number of batches after which a checkpoint of the trained model will be created\n",
        "checkpoint_interval = 2000"
      ],
      "metadata": {
        "id": "z88j1u2PVSXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set command for main function\n",
        "command = \"train\"\n",
        "#command = \"stylize\""
      ],
      "metadata": {
        "id": "9GwukRxSXY-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set command for transformer network type\n",
        "#network = \"transformer_net\"\n",
        "#network = \"transformer_resnext\"\n",
        "network = \"transformer_dense\""
      ],
      "metadata": {
        "id": "fosZKfg0yUMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "def train():\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "    ])\n",
        "    train_dataset = datasets.ImageFolder(train_image_path, transform)\n",
        "    indices = torch.arange(2000)\n",
        "    train_dataset_2k = torch.utils.data.Subset(train_dataset, indices)\n",
        "    train_loader_2k = DataLoader(train_dataset_2k, batch_size=batch_size)\n",
        "    # train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "    if network == \"transformer_dense\":\n",
        "      transformer = TransformerNetworkDenseNet()\n",
        "    elif network == \"transformer_resnext\":\n",
        "      transformer = TransformerResNextNetwork()\n",
        "    else:\n",
        "      transformer = TransformerNet()\n",
        "\n",
        "    optimizer = Adam(transformer.parameters(), learning_rate)\n",
        "    mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "    vgg = VGG16()\n",
        "\n",
        "    # get style features\n",
        "    style_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "    ])\n",
        "    style = load_image(style_image_path)\n",
        "    style = style_transform(style)\n",
        "    style = style.repeat(batch_size, 1, 1, 1)\n",
        "\n",
        "    features_style = vgg(normalize_batch(style))\n",
        "    gram_style = [gram_matrix(y) for y in features_style]\n",
        "\n",
        "    for e in range(num_epochs):\n",
        "        transformer.train()\n",
        "        agg_content_loss = 0.\n",
        "        agg_style_loss = 0.\n",
        "        count = 0\n",
        "        for batch_id, (x, _) in enumerate(train_loader_2k):\n",
        "            n_batch = len(x)\n",
        "            count += n_batch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = x\n",
        "            y = transformer(x)\n",
        "\n",
        "            y = normalize_batch(y)\n",
        "            x = normalize_batch(x)\n",
        "\n",
        "            features_y = vgg(y)\n",
        "            features_x = vgg(x)\n",
        "\n",
        "            content_loss = 1e5 * mse_loss(features_y.relu2_2, features_x.relu2_2)\n",
        "\n",
        "            style_loss = 0.\n",
        "            for ft_y, gm_s in zip(features_y, gram_style):\n",
        "                gm_y = gram_matrix(ft_y)\n",
        "                style_loss += mse_loss(gm_y, gm_s[:n_batch, :, :])\n",
        "            style_loss *= 1e10\n",
        "\n",
        "            total_loss = content_loss + style_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            agg_content_loss += content_loss.item()\n",
        "            agg_style_loss += style_loss.item()\n",
        "\n",
        "            if (batch_id + 1) % log_interval == 0:\n",
        "                mesg = \"{}\\tEpoch {}:\\t[{}/{}]\\tcontent: {:.6f}\\tstyle: {:.6f}\\ttotal: {:.6f}\".format(\n",
        "                    time.ctime(), e + 1, count, len(train_dataset_2k),\n",
        "                                  agg_content_loss / (batch_id + 1),\n",
        "                                  agg_style_loss / (batch_id + 1),\n",
        "                                  (agg_content_loss + agg_style_loss) / (batch_id + 1)\n",
        "                )\n",
        "                print(mesg)\n",
        "\n",
        "    # save model\n",
        "    transformer.eval().cpu()\n",
        "    torch.save(transformer.state_dict(), model_path)\n",
        "\n",
        "    print(\"\\nDone, trained model saved at\", model_path)\n",
        "\n",
        "\n",
        "def stylize():\n",
        "    content_image = load_image(content_image_path)\n",
        "    content_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "    ])\n",
        "    content_image = content_transform(content_image)\n",
        "    content_image = content_image.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        style_model = TransformerNet()\n",
        "        state_dict = torch.load(model_path)\n",
        "        # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
        "        for k in list(state_dict.keys()):\n",
        "            if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
        "                del state_dict[k]\n",
        "        style_model.load_state_dict(state_dict)\n",
        "        style_model\n",
        "        output = style_model(content_image).cpu()\n",
        "\n",
        "    save_image(output_image_path, output[0])\n",
        "\n",
        "def main():\n",
        "    if command == \"train\":\n",
        "        train()\n",
        "    else:\n",
        "        stylize()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "vnuyEMyP0ecu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}